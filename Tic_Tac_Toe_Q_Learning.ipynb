{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ! git clone https://github.com/haje01/gym-tictactoe.git\n",
        "# %cd gym-tictactoe\n",
        "# ! pip install -e ./"
      ],
      "metadata": {
        "id": "oSc3UfwS-fVM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SHAl7ashZpKp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from gym_tictactoe.env import TicTacToeEnv\n",
        "import gym\n",
        "import random\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "J5slY3b3aM86"
      },
      "outputs": [],
      "source": [
        "env = TicTacToeEnv(gym.Env)\n",
        "\n",
        "action_space = env.action_space\n",
        "observable_space = env.observation_space.n\n",
        "\n",
        "q_table = np.zeros([9,9,9,9,9,9,9,9,9])\n",
        "q_table_X = np.zeros([9,9,9,9,9,9,9,9,9])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TeyCqP24afjS"
      },
      "outputs": [],
      "source": [
        "alpha = 0.6 # Learning rate #\n",
        "gamma = 0.6 # Preference for short term reward #\n",
        "epsilon = 0.4 # Preference for exploitation vs exploration #\n",
        "decay_rate= 0.005 # How much it changes to prefer exploitation #\n",
        "\n",
        "episodes = 1 # Number of games played #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-CSX6ig7ah0L"
      },
      "outputs": [],
      "source": [
        "def TicTacToe_Match_Rand(alpha, gamma, epsilon, decay_rate, episodes, q_table):\n",
        "  rewards, epochs, total_penalties = 0, 0, 0\n",
        "\n",
        "  for replay in range(episodes):\n",
        "      state = env.reset() # Resets enviroment at the beggining of a game #\n",
        "      done = False\n",
        "      epochs += 1\n",
        "      env.mark = 'X'\n",
        "\n",
        "      while True:\n",
        "          reward = 0\n",
        "          board = state[0]\n",
        "\n",
        "          if random.uniform(0, 1) < epsilon or env.mark == 'X': # Explore #\n",
        "              action = random.choice(env.available_actions())\n",
        "          else: # Exploit #\n",
        "                action = np.argmax(q_table[board]).round()\n",
        "                if board[action] > 0:\n",
        "                  action = random.choice(env.available_actions())\n",
        "\n",
        "          new_state, reward, done, info = env.step(action) # New variables created as action is executed #\n",
        "          new_board = new_state[0]\n",
        "\n",
        "          rewards += reward\n",
        "          \n",
        "          q_table[board,action] = q_table[board,action] + alpha * (reward + gamma * np.max(q_table[new_board,:])-q_table[board,action])\n",
        "\n",
        "          board = new_state[0] # Updates enviroment #\n",
        "          state = new_state\n",
        "          epsilon = np.exp(-decay_rate * epochs) # Decays epsilon so AI prefers exploitation more over time #\n",
        "\n",
        "          print(f\"Total score: {rewards}\")\n",
        "          print(f\"Score for round: {reward}\")\n",
        "          print(f\"Number of games: {epochs}\")\n",
        "          print(\"\\n\")\n",
        "          env.render()\n",
        "\n",
        "          if done == True:\n",
        "            break\n",
        "  with open('Q-Tic-Tac-Toe.pkl', 'wb') as file:\n",
        "    pickle.dump(q_table, file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def TicTacToe_Match_AI(alpha, gamma, epsilon, decay_rate, episodes, q_table, q_table_X):\n",
        "  rewards, epochs, total_penalties, X_rewards = 0, 0, 0, 0\n",
        "\n",
        "  for replay in range(episodes):\n",
        "      state = env.reset() # Resets enviroment at the beggining of a game #\n",
        "      done = False\n",
        "      epochs += 1\n",
        "      env.mark = 'X'\n",
        "\n",
        "      while True:\n",
        "          reward = 0\n",
        "          board = state[0]\n",
        "\n",
        "          if random.uniform(0, 1) < epsilon: # Explore #\n",
        "              action = random.choice(env.available_actions())\n",
        "          else: # Exploit #\n",
        "                if env.mark == 'X':\n",
        "                  action = np.argmax(q_table[board]).round()\n",
        "                else:\n",
        "                  action = np.argmax(q_table_X[board]).round()\n",
        "\n",
        "                if board[action] > 0:\n",
        "                  action = random.choice(env.available_actions())\n",
        "\n",
        "          new_state, reward, done, info = env.step(action) # New variables created as action is executed #\n",
        "          new_board = new_state[0]\n",
        "\n",
        "          rewards += reward\n",
        "          X_rewards += -reward\n",
        "          \n",
        "          q_table[board,action] = q_table[board,action] + alpha * (reward + gamma * np.max(q_table[new_board,:])-q_table[board,action])\n",
        "          q_table_X[board,action] = q_table_X[board,action] + alpha * (-reward + gamma * np.max(q_table_X[new_board,:])-q_table_X[board,action])\n",
        "\n",
        "\n",
        "          board = new_state[0] # Updates enviroment #\n",
        "          state = new_state\n",
        "          epsilon = np.exp(-decay_rate * epochs) # Decays epsilon so AI prefers exploitation more over time #\n",
        "\n",
        "          print(f\"O's score: {rewards}\")\n",
        "          print(f\"X's score: {X_rewards}\")\n",
        "          print(f\"Number of games: {epochs}\")\n",
        "          print(\"\\n\")\n",
        "          env.render()\n",
        "\n",
        "          if done == True:\n",
        "            break\n",
        "  with open('Q-Tic-Tac-Toe.pkl', 'wb') as file:\n",
        "    pickle.dump(q_table, file)\n",
        "  with open('Q-Tic-Tac-Toe_X.pkl', 'wb') as file:\n",
        "    pickle.dump(q_table_X, file)"
      ],
      "metadata": {
        "id": "d669Qf_VQjqz"
      },
      "execution_count": 10,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Tic-Tac-Toe Q-Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "deepnote": {},
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "57718bd9-f2e8-4ad0-84b9-c26291860047",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}